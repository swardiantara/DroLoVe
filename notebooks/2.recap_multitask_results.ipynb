{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:\\Data Kuliah\\S2 ITS\\Semester 6\\IEEE Access\\DroMoLog\\notebooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.path.abspath(os.curdir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:\\Data Kuliah\\S2 ITS\\Semester 6\\IEEE Access\\DroMoLog\n"
     ]
    }
   ],
   "source": [
    "print(os.path.abspath(os.curdir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "def build_recap_multitask(model, source_dir, output_dir, setting='overall'):\n",
    "    recap_overall = pd.DataFrame()\n",
    "    recap_per_class = pd.DataFrame()\n",
    "    dataset_dirs = os.listdir(source_dir)\n",
    "    for dataset_dir in dataset_dirs:\n",
    "        dataset = dataset_dir\n",
    "        dataset_path = os.path.join(source_dir, dataset_dir)\n",
    "        encoders = os.listdir(dataset_path)\n",
    "        for encoder in encoders:\n",
    "            encoder_path = os.path.join(dataset_path, encoder)\n",
    "            class_weights = os.listdir(encoder_path)\n",
    "            for weight in class_weights:\n",
    "                weight_path = os.path.join(encoder_path, weight)\n",
    "                losses = os.listdir(weight_path)\n",
    "                for loss in losses:\n",
    "                    loss_path = os.path.join(weight_path, loss)\n",
    "                    poolings = os.listdir(loss_path)\n",
    "                    for pooling in poolings:\n",
    "                        pool_path = os.path.join(loss_path, pooling)\n",
    "                        n_layers = os.listdir(pool_path)\n",
    "                        for n_layer in n_layers:\n",
    "                            layer_path = os.path.join(pool_path, n_layer)\n",
    "                            n_heads = os.listdir(layer_path)\n",
    "                            for n_head in n_heads:\n",
    "                                head_path = os.path.join(layer_path, n_head)\n",
    "                                bidirectionals = os.listdir(head_path)\n",
    "                                for bidirectional in bidirectionals:\n",
    "                                    bidirectional_path = os.path.join(head_path, bidirectional)\n",
    "                                    files = os.listdir(bidirectional_path)\n",
    "                                    eval_report_path = os.path.join(bidirectional_path, files[1])\n",
    "                                    scenario_path = os.path.join(bidirectional_path, files[4])\n",
    "                                    with open(eval_report_path) as eval_report_file:\n",
    "                                        eval_report = json.load(eval_report_file)\n",
    "                                        with open(scenario_path) as scenario_json_file:\n",
    "                                            scenario = json.load(scenario_json_file)\n",
    "                                            if setting == 'overall':\n",
    "                                                prediction_path = os.path.join(bidirectional_path, 'prediction.xlsx')\n",
    "                                                if os.path.exists(prediction_path):\n",
    "                                                    prediction_df = pd.read_excel(prediction_path)\n",
    "                                                else:\n",
    "                                                    prediction_df = pd.read_csv(os.path.join(bidirectional_path, 'prediction.csv'))\n",
    "                                                if 'prob' not in prediction_df.columns:\n",
    "                                                    prediction_df['prob'] = [row[f\"prob_{row['label']}\"] for _, row in prediction_df.iterrows()]\n",
    "                                                    prediction_df.to_csv(bidirectional_path, 'prediction.xlsx', index=False)\n",
    "                                                \n",
    "                                                confidence_avg = prediction_df['prob'].mean()\n",
    "                                                confidence_std = prediction_df['prob'].std()\n",
    "                                                accuracy = float(round(eval_report['accuracy'] * 100, 5))\n",
    "                                                precision = float(round(eval_report['weighted_avg']['precision'] * 100, 5))\n",
    "                                                recall = float(round(eval_report['weighted_avg']['recall'] * 100, 5))\n",
    "                                                f1_score = float(round(eval_report['weighted_avg']['f1-score'] * 100, 5))\n",
    "                                                best_epoch = int(scenario['best_epoch'])\n",
    "                                                columns = ['dataset', 'encoder', 'n_layer', 'n_head', 'bidirectional', 'pooling', 'class_weight', 'scenario', 'best_epoch', 'accuracy', 'precision', 'recall', 'f1_score', 'confidence_avg', 'confidence_std']\n",
    "                                                row_list = [dataset, encoder, n_layer, n_head, bidirectional, pooling, weight, loss, best_epoch, accuracy, precision, recall, f1_score, confidence_avg, confidence_std]\n",
    "                                                scenario_df = pd.DataFrame([row_list], index=None, columns=columns)\n",
    "                                                recap_overall = pd.concat([recap_overall, scenario_df], ignore_index=True)\n",
    "                                                if not os.path.exists(output_dir):\n",
    "                                                    os.makedirs(output_dir)\n",
    "                                                recap_overall.to_excel(os.path.join(output_dir, f'recap_overall_{model}.xlsx'), index=False, float_format='%.5f')\n",
    "                                            else:\n",
    "                                                accuracy = float(round(eval_report['accuracy'] * 100, 5))\n",
    "                                                best_epoch = int(scenario['best_epoch'])\n",
    "                                                high_score = [float(round(eval_report['high'][key] * 100, 5)) for key in eval_report['high'].keys()]\n",
    "                                                low_score = [float(round(eval_report['low'][key] * 100, 5)) for key in eval_report['low'].keys()]\n",
    "                                                medium_score = [float(round(eval_report['medium'][key] * 100, 5)) for key in eval_report['medium'].keys()]\n",
    "                                                normal_score = [float(round(eval_report['normal'][key] * 100, 5)) for key in eval_report['normal'].keys()]\n",
    "                                                columns = ['dataset', 'encoder', 'n_layer', 'n_head', 'bidirectional', 'pooling', 'class_weight', 'scenario', 'best_epoch', 'accuracy', 'high_precision', 'high_recall', 'high_f1_score', 'medium_precision', 'medium_recall', 'medium_f1_score', 'low_precision', 'low_recall', 'low_f1_score', 'normal_precision', 'normal_recall', 'normal_f1_score']\n",
    "                                                row_list = [dataset, encoder, n_layer, n_head, bidirectional, pooling, weight, loss, best_epoch, accuracy, high_score[0], high_score[1], high_score[2], medium_score[0], medium_score[1], medium_score[2], low_score[0], low_score[1], low_score[2], normal_score[0], normal_score[1], normal_score[2]]\n",
    "                                                scenario_df = pd.DataFrame([row_list], index=None, columns=columns)\n",
    "                                                recap_per_class = pd.concat([recap_per_class, scenario_df], ignore_index=True)\n",
    "                                                recap_per_class.to_excel(os.path.join(output_dir, f'recap_per_class_{model}.xlsx'), index=False, float_format='%.5f')\n",
    "    return recap_overall, recap_per_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our Own Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'multitask_15'\n",
    "source_dir = os.path.join('experiments', model)\n",
    "output_dir = os.path.join('experiments', 'recap', model)\n",
    "build_recap_multitask(model, source_dir, output_dir, 'overall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'baseline'\n",
    "source_dir = os.path.join('experiments', model)\n",
    "output_dir = os.path.join('experiments', 'recap', model)\n",
    "build_recap_multitask(model, source_dir, output_dir, 'per_class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### State-of-the-art Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'transsentlog'\n",
    "source_dir = os.path.join('experiments', model)\n",
    "output_dir = os.path.join('experiments', 'recap', model)\n",
    "build_recap_multitask(model, source_dir, output_dir, 'overall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'transsentlog'\n",
    "source_dir = os.path.join('experiments', model)\n",
    "output_dir = os.path.join('experiments', 'recap', model)\n",
    "build_recap_multitask(model, source_dir, output_dir, 'per_class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'neurallog'\n",
    "source_dir = os.path.join('experiments', model)\n",
    "output_dir = os.path.join('experiments', 'recap', model)\n",
    "build_recap_multitask(model, source_dir, output_dir, 'overall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'neurallog'\n",
    "source_dir = os.path.join('experiments', model)\n",
    "output_dir = os.path.join('experiments', 'recap', model)\n",
    "build_recap_multitask(model, source_dir, output_dir, 'per_class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ablation Study Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'freeze_bert_15'\n",
    "source_dir = os.path.join('experiments', model)\n",
    "output_dir = os.path.join('experiments', 'recap', model)\n",
    "build_recap_multitask(model, source_dir, output_dir, 'overall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'exclude_cls_15'\n",
    "source_dir = os.path.join('experiments', model)\n",
    "output_dir = os.path.join('experiments', 'recap', model)\n",
    "build_recap_multitask(model, source_dir, output_dir, 'overall')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Effect of Batch Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      dataset      encoder n_layer n_head   bidirectional pooling  \\\n",
      "0    filtered         lstm       2      1  unidirectional    last   \n",
      "1    filtered  transformer       1      6  unidirectional     avg   \n",
      "2  unfiltered         lstm       2      1  unidirectional    last   \n",
      "3  unfiltered  transformer       1      6  unidirectional     avg   \n",
      "\n",
      "  class_weight scenario  best_epoch  accuracy  precision    recall  f1_score  \\\n",
      "0      inverse  logloss           8  81.19658   82.07327  81.19658  80.37834   \n",
      "1     balanced  logloss           4  80.34188   80.43424  80.34188  80.28081   \n",
      "2      inverse  logloss          15  94.64286   95.93166  94.64286  95.13227   \n",
      "3     balanced  logloss           8  95.75893   96.32785  95.75893  95.94623   \n",
      "\n",
      "   confidence_avg  confidence_std  batch_size  \n",
      "0        0.965211        0.083777           4  \n",
      "1        0.939968        0.116998           4  \n",
      "2        0.995753        0.021641           4  \n",
      "3        0.996535        0.022354           4  \n",
      "      dataset      encoder n_layer n_head   bidirectional pooling  \\\n",
      "0    filtered         lstm       2      1  unidirectional    last   \n",
      "1    filtered  transformer       1      6  unidirectional     avg   \n",
      "2  unfiltered         lstm       2      1  unidirectional    last   \n",
      "3  unfiltered  transformer       1      6  unidirectional     avg   \n",
      "\n",
      "  class_weight scenario  best_epoch  accuracy  precision    recall  f1_score  \\\n",
      "0      inverse  logloss          13  82.05128   81.69886  82.05128  81.85873   \n",
      "1     balanced  logloss           7  79.48718   78.99526  79.48718  79.04951   \n",
      "2      inverse  logloss          15  95.31250   95.77885  95.31250  95.48681   \n",
      "3     balanced  logloss           7  96.87500   96.85586  96.87500  96.85079   \n",
      "\n",
      "   confidence_avg  confidence_std  batch_size  \n",
      "0        0.976400        0.071148           8  \n",
      "1        0.966764        0.080716           8  \n",
      "2        0.996915        0.020759           8  \n",
      "3        0.995128        0.027091           8  \n",
      "      dataset      encoder n_layer n_head   bidirectional pooling  \\\n",
      "0    filtered         lstm       2      1  unidirectional    last   \n",
      "1    filtered  transformer       1      6  unidirectional     avg   \n",
      "2  unfiltered         lstm       2      1  unidirectional    last   \n",
      "3  unfiltered  transformer       1      6  unidirectional     avg   \n",
      "\n",
      "  class_weight scenario  best_epoch  accuracy  precision    recall  f1_score  \\\n",
      "0      inverse  logloss          12  75.21368   75.08479  75.21368  74.48119   \n",
      "1     balanced  logloss           5  78.63248   77.67423  78.63248  77.98653   \n",
      "2      inverse  logloss          13  94.41964   95.68306  94.41964  94.90986   \n",
      "3     balanced  logloss          11  95.53571   95.97609  95.53571  95.70489   \n",
      "\n",
      "   confidence_avg  confidence_std  batch_size  \n",
      "0        0.970057        0.066327          16  \n",
      "1        0.940471        0.105722          16  \n",
      "2        0.992607        0.045741          16  \n",
      "3        0.995086        0.033965          16  \n",
      "      dataset      encoder n_layer n_head   bidirectional pooling  \\\n",
      "0    filtered         lstm       2      1  unidirectional    last   \n",
      "1    filtered  transformer       1      6  unidirectional     avg   \n",
      "2  unfiltered         lstm       2      1  unidirectional    last   \n",
      "3  unfiltered  transformer       1      6  unidirectional     avg   \n",
      "\n",
      "  class_weight scenario  best_epoch  accuracy  precision    recall  f1_score  \\\n",
      "0      inverse  logloss          12  77.77778   77.33003  77.77778  77.16976   \n",
      "1     balanced  logloss           5  78.63248   79.04112  78.63248  78.47240   \n",
      "2      inverse  logloss          11  94.19643   95.38367  94.19643  94.68315   \n",
      "3     balanced  logloss          11  95.31250   96.21853  95.31250  95.65214   \n",
      "\n",
      "   confidence_avg  confidence_std  batch_size  \n",
      "0        0.966293        0.055021          32  \n",
      "1        0.911551        0.133560          32  \n",
      "2        0.991899        0.038912          32  \n",
      "3        0.994284        0.037399          32  \n",
      "      dataset      encoder n_layer n_head   bidirectional pooling  \\\n",
      "0    filtered         lstm       2      1  unidirectional    last   \n",
      "1    filtered  transformer       1      6  unidirectional     avg   \n",
      "2  unfiltered         lstm       2      1  unidirectional    last   \n",
      "3  unfiltered  transformer       1      6  unidirectional     avg   \n",
      "\n",
      "  class_weight scenario  best_epoch  accuracy  precision    recall  f1_score  \\\n",
      "0      inverse  logloss          15  76.06838   75.56482  76.06838  75.61977   \n",
      "1     balanced  logloss           9  76.06838   76.88891  76.06838  76.07641   \n",
      "2      inverse  logloss          13  94.19643   95.08487  94.19643  94.56047   \n",
      "3     balanced  logloss          10  95.08929   95.47773  95.08929  95.23278   \n",
      "\n",
      "   confidence_avg  confidence_std  batch_size  \n",
      "0        0.960916        0.071726          64  \n",
      "1        0.932330        0.117665          64  \n",
      "2        0.992423        0.031060          64  \n",
      "3        0.987346        0.059377          64  \n",
      "      dataset      encoder n_layer n_head   bidirectional pooling  \\\n",
      "0    filtered         lstm       2      1  unidirectional    last   \n",
      "1    filtered  transformer       1      6  unidirectional     avg   \n",
      "2  unfiltered         lstm       2      1  unidirectional    last   \n",
      "3  unfiltered  transformer       1      6  unidirectional     avg   \n",
      "\n",
      "  class_weight scenario  best_epoch  accuracy  precision    recall  f1_score  \\\n",
      "0      inverse  logloss          15  60.68376   66.43214  60.68376  62.67385   \n",
      "1     balanced  logloss          14  76.06838   76.30882  76.06838  76.04550   \n",
      "2      inverse  logloss          15  94.20935   96.19002  94.20935  94.97169   \n",
      "3     balanced  logloss          13  95.99109   96.76962  95.99109  96.28640   \n",
      "\n",
      "   confidence_avg  confidence_std  batch_size  \n",
      "0        0.832284        0.146124         128  \n",
      "1        0.943017        0.117228         128  \n",
      "2        0.986656        0.047510         128  \n",
      "3        0.989062        0.050852         128  \n"
     ]
    }
   ],
   "source": [
    "batch_sizes = [4, 8, 16, 32, 64, 128]\n",
    "batch_size_df = pd.DataFrame()\n",
    "for batch in batch_sizes:\n",
    "    model = f'batch_size_{batch}_15'\n",
    "    source_dir = os.path.join('experiments', model)\n",
    "    output_dir = os.path.join('experiments', 'recap', model)\n",
    "    batch_df, _ = build_recap_multitask(model, source_dir, output_dir, 'overall')\n",
    "    batch_df['batch_size'] = batch\n",
    "    print(batch_df)\n",
    "    batch_size_df = pd.concat([batch_size_df, batch_df], ignore_index=True)\n",
    "\n",
    "# print(batch_size_df)\n",
    "output_dir = os.path.join('experiments', 'recap', 'batch_size')\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "batch_size_df.to_excel(os.path.join(output_dir, f'recap_overall_batch_size.xlsx'), index=False, float_format='%.5f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
